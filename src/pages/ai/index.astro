---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout title="AI">
  <section class="text-page">
    <h1>My First LoRA Training — And How It Changed the Way I Think About AI</h1>
    <p>This was my first LoRA training.</p>
    <p>I didn’t approach it as a technical experiment alone. I approached it as a question:</p>
    <p>If I compress my own visual language into a diffusion model, what remains mine?</p>
    <p>
      There’s a common assumption around AI image generation — that once the system runs, human
      agency fades. That the machine “creates” and the artist merely triggers.
    </p>
    <p>What I discovered was the opposite.</p>
    <p>The more I understood the system, the more deliberate my authorship became.</p>

    <h2>Starting With a Coherent Visual Language</h2>
    <p>I selected nine hand-drawn portraits from my own series.</p>
    <p>They shared a clear structural logic:</p>
    <p class="u-pl-2"><span>&#8226;</span> Large abstract color-wash backgrounds</p>
    <p class="u-pl-2"><span>&#8226;</span> Soft, pencil-like rendering</p>
    <p class="u-pl-2"><span>&#8226;</span> High-contrast topographic contour lines</p>
    <p class="u-pl-2 u-pb-1"><span>&#8226;</span> Controlled but expressive color palettes</p>
    <p>I wasn’t aiming for dataset diversity.</p>
    <p>I was aiming for stylistic consistency.</p>
    <p>
      If the internal language of the series was strong enough, the model would not learn subjects
      — it would learn structure.
    </p>
    <div class="text-image">
      <img src="/art/dataset.png" alt="" loading="lazy" />
    </div>

    <h2>Building and Training Locally</h2>
    <p>The training ran locally on:</p>
    <p class="u-pl-2"><span>&#8226;</span>Apple M1 Max (64GB RAM)</p>
    <p class="u-pl-2"><span>&#8226;</span>ComfyUI</p>
    <p class="u-pl-2"><span>&#8226;</span>Z-Image base model</p>
    <p class="u-pl-2 u-pb-1"><span>&#8226;</span>Musubi Tuner for LoRA training</p>
    <p>Final training configuration:</p>
    <p class="u-pl-2"><span>&#8226;</span>1000 steps</p>
    <p class="u-pl-2"><span>&#8226;</span>Learning rate: 0.0001</p>
    <p class="u-pl-2"><span>&#8226;</span>LoRA rank: 16</p>
    <p class="u-pl-2"><span>&#8226;</span>fp16 precision</p>
    <p class="u-pl-2"><span>&#8226;</span>~53 hours training time</p>
    <p>
      It required environment tuning, precision adjustments, and model compatibility fixes. Nothing
      about it was one-click.
    </p>
    <p>But that technical depth was important.</p>
    <p>It made the process intentional.</p>

    <h2>Designing the Dataset to Teach Style, Not Content</h2>
    <p>All image captions were nearly identical.</p>
    <p>Only the subject label changed: woman / man / child.</p>
    <p>Everything else remained stable.</p>
    <p>This forced the model to extract: line behavior, color layering, facial abstraction, background logic</p>

    <p>The LoRA was named: <i>sportrait-laz</i></p>
    <p>And when I first applied it during generation, I wasn’t looking for replication.</p>
    <p>I was looking for structural resonance.</p>

    <h2>The First Outputs</h2>
    <p>At LoRA strength values around 1.2–1.3, the influence became visible.</p>
    <p>The generated images began to show: dominant pencil-like line work, contour mapping along facial planes, layered atmospheric backgrounds</p>

    <p>It wasn’t copying specific drawings.</p>
    <p>It was reconstructing tendencies embedded across the series.</p>

    <div class="text-image">
      <img src="/art/firstloras.png" alt="" loading="lazy" />
    </div>

    <p>That moment changed how I saw the system.</p>
    <p>It wasn’t autonomous.</p>
    <p>It was reactive to the structure I had curated.</p>

    <h2>Where Agency Actually Lives</h2>

    <p>The real work started after training.</p>
    <p>Control emerged through:</p>
    <p class="u-pl-2"><span>&#8226;</span>LoRA strength (how dominant the trained style is)</p>
    <p class="u-pl-2"><span>&#8226;</span>CFG (how strongly the model follows prompts)</p>
    <p class="u-pl-2"><span>&#8226;</span>Prompt structure</p>
    <p class="u-pl-2"><span>&#8226;</span>Iterative comparison</p>
    <p class="u-pl-2"><span>&#8226;</span>Lower CFG + moderate LoRA created openness and variation.</p>
    <p class="u-pl-2"><span>&#8226;</span>Higher CFG + stronger LoRA sharpened structure — sometimes too much.</p>

    <p>Prompt phrasing became a compositional tool:</p>
    <p class="u-pl-2"><span>&#8226;</span>“line-driven portrait”</p>
    <p class="u-pl-2"><span>&#8226;</span>“drawing-first composition”</p>
    <p class="u-pl-2"><span>&#8226;</span>“high-contrast luminous contour lines”</p>
    <p>Excluding:</p>
    <p class="u-pl-2"><span>&#8226;</span>photorealistic</p>
    <p class="u-pl-2"><span>&#8226;</span>smooth shading</p>
    <p class="u-pl-2"><span>&#8226;</span>airbrushed</p>
    <p class="u-pb-1">prevented stylistic drift.</p>
    <p>The workflow became:</p>
    <p>Generate → Evaluate → Adjust → Repeat.</p>
    <p>This was not passive triggering.</p>
    <p>It was art direction.</p>

    <div class="text-image">
      <img src="/art/linework.png" alt="" loading="lazy" />
    </div>

    <h2>Sculpting the Image Space</h2>
    <p>Once line dominance stabilized, I explored chromatic control:</p>
    <p class="u-pl-2"><span>&#8226;</span>Blue-dominant backgrounds</p>
    <p class="u-pl-2"><span>&#8226;</span>Muted violet atmospheres</p>
    <p class="u-pl-2"><span>&#8226;</span>Desaturated tonal fields</p>
    <p class="u-pl-2 u-pb-1"><span>&#8226;</span>Adjusted brightness levels</p>
    <p>Small textual changes led to significant visual shifts.</p>
    <p>Some variations failed.</p>
    <p>Some revealed unexpected potential.</p>
    <p>Iteration wasn’t correction — it was refinement.</p>

    <div class="text-image">
      <img src="/art/backgroundexperiments.png" alt="" loading="lazy" />
    </div>

    <h2>What Changed in My Understanding of AI</h2>
    <p>Before this project, AI felt like a generator.</p>
    <p>After this training, it felt like a structured environment.</p>
    <p>Agency did not disappear. It relocated.</p>

    <p>It exists in:</p>
    <p class="u-pl-2"><span>&#8226;</span>Dataset curation</p>
    <p class="u-pl-2"><span>&#8226;</span>Caption strategy</p>
    <p class="u-pl-2"><span>&#8226;</span>Parameter balancing</p>
    <p class="u-pl-2"><span>&#8226;</span>Iterative selection</p>
    <p class="u-pl-2"><span>&#8226;</span>Visual comparison</p>
    <p class="u-pl-2 u-pb-1"><span>&#8226;</span>Constraint setting</p>
    <p>The model doesn’t replace authorship.</p>
    <p>It responds to it.</p>
    <p>The outputs reflect the coherence — or incoherence — of the input logic.</p>

    <div class="text-image">
      <img src="/art/finalloras.png" alt="" loading="lazy" />
    </div>

    <h2>What This First Training Taught Me</h2>

    <p>Style can be compressed into a model if the dataset is coherent.</p>
    <p>Parameter control is a creative instrument.</p>
    <p>Iteration is where authorship becomes visible.</p>
    <p>The system amplifies structural decisions.</p>
    <p>Human agency is not diminished — it becomes procedural.</p>
    <p>This was my first LoRA training.</p>
    <p>It didn’t make AI feel more powerful.</p>
    <p>It made artistic control feel more explicit.</p>
    <p>And that changed how I think about working with these systems entirely.</p>
    <p>
      If you’re interested in a deeper breakdown of the technical setup, parameter decisions, visual
      comparisons, and the full art direction process behind this project, I’ve documented
      everything in detail in the essay published in the
      <a class="text-link" href="/writings">Writings</a> section on my website. The paper traces the
      training configuration, iteration logic, and selection process step by step.
    </p>

    <p>The work is still ongoing: I’m currently expanding the portrait series, creating additional drawings to
    strengthen dataset consistency, and refining the LoRA through further training runs to improve stylistic
    coherence and facial variation. This isn’t a finished system — it’s an evolving visual language.</p>
  </section>
</BaseLayout>

<style>
  .text-page {
    max-width: 820px;
    display: grid;
    gap: calc(var(--u) * 2);
    font-size: var(--main-font);
    margin: 0 auto;
    text-align: left;
  }

  .text-page p {
    font-size: 14px;
  }

  .text-image {
    width: 100%;
  }

  .text-image img {
    width: 100%;
    display: block;
    height: auto;
  }

  @media (min-width: 769px) {
    .text-image img {
      width: 70%;
      margin: 0 auto;
    }
  }

  .text-link {
    font-weight: 700 !important;
  }

  .text-link:hover {
    text-decoration: underline;
    text-decoration-thickness: 0.12em;
    text-underline-offset: 0.18em;
  }

  .text-page h2 {
    font-size: 16px;
    padding: calc(var(--u) * 1) 0;
  }

  .text-page h1 {
    font-size: var(--main-font);
    padding-bottom: calc(var(--u) * 3);
  }
</style>
